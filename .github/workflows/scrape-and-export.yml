name: Scrape and Export Data

on:
  schedule:
    # Run daily at 2 AM UTC (adjust timezone as needed)
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual trigger from GitHub UI

jobs:
  scrape-and-export:
    runs-on: ubuntu-latest
    
    steps:
      - name: "FORCE CHECKOUT LATEST CODE + PROOF"
        uses: actions/checkout@v4
        with:
          ref: ${{ github.head_ref || github.ref || 'main' }}
          fetch-depth: 0
          clean: true
      
      - name: "CRITICAL DIAGNOSTIC - Verify exact environment"
        run: |
          echo "=== GIT INFORMATION ==="
          echo "Commit: $(git rev-parse HEAD)"
          echo "Branch: $(git branch --show-current)"
          echo "Remote: $(git remote get-url origin)"
          echo ""
          echo "=== PYTHON ENVIRONMENT ==="
          python --version
          python -c "import sys; print('Python path:'); [print(f'  {p}') for p in sys.path]"
          echo ""
          echo "=== SCHEMA.PY VERIFICATION ==="
          echo "File exists: $(test -f scraper/schema.py && echo 'YES' || echo 'NO')"
          echo "File size: $(stat -f%z scraper/schema.py 2>/dev/null || stat -c%s scraper/schema.py 2>/dev/null || echo 'unknown')"
          echo "First 10 lines:"
          head -10 scraper/schema.py
          echo ""
          echo "=== MODULE IMPORT TEST ==="
          python -c "
          import sys
          import os
          sys.path.insert(0, os.getcwd())
          from scraper.schema import init_db
          import inspect
          print(f'Module file: {inspect.getfile(init_db)}')
          print(f'Module line: {inspect.getsourcelines(init_db)[1]}')
          print(f'Has _construct_sqlite_url: {\"_construct_sqlite_url\" in inspect.getsource(init_db)}')
          " || echo "ERROR: Failed to import or inspect module"
          echo ""
          echo "=== CLEAR PYTHON CACHE ==="
          find . -type d -name "__pycache__" -exec rm -r {} + 2>/dev/null || true
          find . -name "*.pyc" -delete 2>/dev/null || true
          echo "Python cache cleared"
      
      - name: "VERIFY schema.py IS REALLY NEW"
        run: |
          echo "Commit being used: $(git rev-parse HEAD)"
          echo "=== First 30 lines of scraper/schema.py ==="
          head -30 scraper/schema.py
          echo "=== Searching for the fix ==="
          grep -n "_construct_sqlite_url\|Using database URL" scraper/schema.py && echo "NEW CODE FOUND" || echo "OLD CODE STILL HERE – THIS IS THE PROBLEM"
      
      - name: "CRITICAL: Show exactly which schema.py is on disk"
        run: |
          echo "Current commit:"
          git rev-parse HEAD
          echo ""
          echo "=== File timestamp ==="
          ls -la scraper/schema.py
          echo ""
          echo "=== First 25 lines of scraper/schema.py ==="
          head -25 scraper/schema.py
          echo ""
          echo "=== Does the fix exist? ==="
          grep -n "_construct_sqlite_url\|Using database URL\|init_db(" scraper/schema.py || echo "FIX IS MISSING – OLD FILE IS BEING USED!"
          echo ""
          echo "If you don't see '_construct_sqlite_url' above → GitHub Actions is using an old commit!"
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            wget \
            gnupg \
            unzip
      
      - name: Install Chrome for browser automation
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          google-chrome --version
      
      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: DEBUG – Show which schema.py is really being used
        run: |
          echo "=== Content of scraper/schema.py on the runner ==="
          cat scraper/schema.py
          echo ""
          echo "=== Does it contain our fix? ==="
          grep -n "_construct_sqlite_url\|Using database URL\|FIXED VERSION" scraper/schema.py || echo "FIX NOT PRESENT – OLD FILE!"
          echo ""
          echo "=== First 10 lines ==="
          head -10 scraper/schema.py
          echo ""
          echo "=== Git commit info ==="
          git log --oneline -5
          echo ""
          echo "=== Current branch ==="
          git branch --show-current
      
      - name: Verify schema.py contains the fix
        run: |
          echo "=== First 5 lines of scraper/schema.py ==="
          head -5 scraper/schema.py
          echo ""
          echo "=== Looking for _construct_sqlite_url function ==="
          grep -n "_construct_sqlite_url" scraper/schema.py || (echo "ERROR: NOT FOUND – OLD VERSION STILL IN USE!" && exit 1)
          echo ""
          echo "=== Looking for debug marker ==="
          grep -n "USING NEW FIXED VERSION" scraper/schema.py || (echo "ERROR: Debug marker not found!" && exit 1)
          echo ""
          echo "=== CRITICAL: Testing init_db() version check ==="
          python -c "from scraper.schema import init_db; print('✓ init_db() imported successfully'); print('✓ Version check passed - new code confirmed')" || (echo "ERROR: Version check failed - old code is running!" && exit 1)
          echo "✓ schema.py verification passed - new version confirmed"
      
      - name: Initialize database
        env:
          SECRET_KEY: ${{ secrets.SECRET_KEY }}
          JWT_SECRET_KEY: ${{ secrets.JWT_SECRET_KEY }}
          DATABASE_URL: sqlite:///social_media.db
        run: |
          echo "=== WORKFLOW VERSION: Using diagnostic script with comprehensive error handling - 2025-01-24 ==="
          echo "Working directory: $(pwd)"
          echo "Python version: $(python --version)"
          echo ""
          python scripts/init_db_ci.py || {
            echo ""
            echo "=== FALLBACK: Trying direct init_db() call with explicit SQLite URL ==="
            python -c "
            import os
            import sys
            from pathlib import Path
            sys.path.insert(0, str(Path.cwd()))
            
            try:
                from scraper.schema import init_db, Base
                from sqlalchemy import create_engine
                
                # Use explicit SQLite URL format (as recommended)
                db_url = 'sqlite:///social_media.db'
                print(f'Attempting with explicit SQLite URL: {db_url}')
                engine = init_db(db_url)
                print('✓ Success with explicit SQLite URL')
                engine.dispose()
            except Exception as e1:
                print(f'Explicit URL failed: {e1}')
                try:
                    # Fallback: absolute path with SQLite URL
                    db_path = os.path.abspath('social_media.db')
                    db_url = f'sqlite:///{db_path.replace(os.sep, \"/\")}'
                    print(f'Attempting with absolute SQLite URL: {db_url}')
                    engine = init_db(db_url)
                    print('✓ Success with absolute SQLite URL')
                    engine.dispose()
                except Exception as e2:
                    print(f'Absolute URL failed: {e2}')
                    # Last resort: direct creation with explicit URL
                    print('Attempting direct Base.metadata.create_all() with explicit URL')
                    db_path = os.path.abspath('social_media.db')
                    url = f'sqlite:///{db_path.replace(os.sep, \"/\")}'
                    engine = create_engine(url, future=True)
                    Base.metadata.create_all(engine)
                    print('✓ Success with direct creation')
                    engine.dispose()
            "
          }
      
      - name: Populate accounts
        env:
          SECRET_KEY: ${{ secrets.SECRET_KEY }}
          JWT_SECRET_KEY: ${{ secrets.JWT_SECRET_KEY }}
        run: |
          python -c "from scraper.extract_accounts import populate_accounts; populate_accounts()"
      
      - name: Run scraper
        env:
          SECRET_KEY: ${{ secrets.SECRET_KEY }}
          JWT_SECRET_KEY: ${{ secrets.JWT_SECRET_KEY }}
          DATABASE_PATH: social_media.db
        run: |
          python -c "from scraper.collect_metrics import simulate_metrics; simulate_metrics(mode='real', parallel=True, max_workers=5)"
      
      - name: Create public directory
        run: |
          mkdir -p public/data
      
      - name: Export to CSV
        run: |
          python scripts/export_to_csv.py social_media.db public/data/hhs_social_media_data.csv
      
      - name: Export to JSON
        run: |
          python scripts/export_to_json.py social_media.db public/data/hhs_social_media_data.json
      
      - name: Generate report
        run: |
          python scripts/generate_report.py social_media.db > public/report.txt || echo "Report generation completed with warnings"
      
      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add public/
          git diff --staged --quiet || git commit -m "Update data export: $(date +'%Y-%m-%d %H:%M:%S UTC')"
          git push

