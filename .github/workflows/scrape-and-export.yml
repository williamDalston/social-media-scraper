name: Scrape and Export Data

on:
  schedule:
    # Run daily at 2 AM UTC (adjust timezone as needed)
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual trigger from GitHub UI

jobs:
  scrape-and-export:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code (force latest commit)
        uses: actions/checkout@v4
        with:
          ref: ${{ github.head_ref || github.ref }}   # PR head or branch
          fetch-depth: 0                              # full history
          clean: true                                 # clean workspace
      
      - name: "CRITICAL: Show exactly which schema.py is on disk"
        run: |
          echo "Current commit:"
          git rev-parse HEAD
          echo ""
          echo "=== File timestamp ==="
          ls -la scraper/schema.py
          echo ""
          echo "=== First 25 lines of scraper/schema.py ==="
          head -25 scraper/schema.py
          echo ""
          echo "=== Does the fix exist? ==="
          grep -n "_construct_sqlite_url\|Using database URL\|init_db(" scraper/schema.py || echo "FIX IS MISSING – OLD FILE IS BEING USED!"
          echo ""
          echo "If you don't see '_construct_sqlite_url' above → GitHub Actions is using an old commit!"
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            wget \
            gnupg \
            unzip
      
      - name: Install Chrome for browser automation
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          google-chrome --version
      
      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: DEBUG – Show which schema.py is really being used
        run: |
          echo "=== Content of scraper/schema.py on the runner ==="
          cat scraper/schema.py
          echo ""
          echo "=== Does it contain our fix? ==="
          grep -n "_construct_sqlite_url\|Using database URL\|FIXED VERSION" scraper/schema.py || echo "FIX NOT PRESENT – OLD FILE!"
          echo ""
          echo "=== First 10 lines ==="
          head -10 scraper/schema.py
          echo ""
          echo "=== Git commit info ==="
          git log --oneline -5
          echo ""
          echo "=== Current branch ==="
          git branch --show-current
      
      - name: Verify schema.py contains the fix
        run: |
          echo "=== First 5 lines of scraper/schema.py ==="
          head -5 scraper/schema.py
          echo ""
          echo "=== Looking for _construct_sqlite_url function ==="
          grep -n "_construct_sqlite_url" scraper/schema.py || (echo "ERROR: NOT FOUND – OLD VERSION STILL IN USE!" && exit 1)
          echo ""
          echo "=== Looking for debug marker ==="
          grep -n "USING NEW FIXED VERSION" scraper/schema.py || (echo "ERROR: Debug marker not found!" && exit 1)
          echo "✓ schema.py verification passed - new version confirmed"
      
      - name: Initialize database
        env:
          SECRET_KEY: ${{ secrets.SECRET_KEY }}
          JWT_SECRET_KEY: ${{ secrets.JWT_SECRET_KEY }}
          DATABASE_URL: sqlite:///social_media.db
        run: |
          # Use explicit SQLite URL format (init_db also handles bare filenames, but explicit is safer)
          python -c "from scraper.schema import init_db; init_db('sqlite:///social_media.db')"
          python -c "from scraper.extract_accounts import populate_accounts; populate_accounts()"
      
      - name: Run scraper
        env:
          SECRET_KEY: ${{ secrets.SECRET_KEY }}
          JWT_SECRET_KEY: ${{ secrets.JWT_SECRET_KEY }}
          DATABASE_PATH: social_media.db
        run: |
          python -c "from scraper.collect_metrics import simulate_metrics; simulate_metrics(mode='real', parallel=True, max_workers=5)"
      
      - name: Create public directory
        run: |
          mkdir -p public/data
      
      - name: Export to CSV
        run: |
          python scripts/export_to_csv.py social_media.db public/data/hhs_social_media_data.csv
      
      - name: Export to JSON
        run: |
          python scripts/export_to_json.py social_media.db public/data/hhs_social_media_data.json
      
      - name: Generate report
        run: |
          python scripts/generate_report.py social_media.db > public/report.txt || echo "Report generation completed with warnings"
      
      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add public/
          git diff --staged --quiet || git commit -m "Update data export: $(date +'%Y-%m-%d %H:%M:%S UTC')"
          git push

